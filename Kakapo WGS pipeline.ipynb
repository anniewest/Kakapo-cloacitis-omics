{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1d2746",
   "metadata": {},
   "source": [
    "# WGS sequencing data-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614dbeaf",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff6644",
   "metadata": {},
   "source": [
    "**Index**\n",
    "\n",
    "1. QC, trimming, and filtering\n",
    "   - Quality trimming (Trimmomatic)\n",
    "   - Re-check quality (FastQC; MultiQC)\n",
    "   - Host filtering\n",
    "   - Check library sizes for each step\n",
    "2. Assembly\n",
    "   - Co-assembly\n",
    "   - Per-sample assemblies\n",
    "   - Assessing assemblies\n",
    "3. Dereplicating contigs across multiple assemblies\n",
    "4. Gene prediction of dereplicated contigs\n",
    "5. Gene annotation of dereplicated contigs\n",
    "6. Read mapping\n",
    "   - Generate read mapping index\n",
    "   - WGS per-sample read mapping\n",
    "   - WTS per-sample read mapping\n",
    "7. Assign mapped reads to predicted genes\n",
    "   - Extract gene coordinates to generate gene_coords SAF file\n",
    "   - featureCounts: WGS data\n",
    "   - featureCounts: WTS data\n",
    "8. Per-sample coverage calculations\n",
    "   - Coverage calculations overview\n",
    "   - WGS: per-sample DNA read coverage per gene\n",
    "   - WTS: per-sample RNA read coverage per gene\n",
    "9. Final notes\n",
    "   - Key output files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3574f9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db83984",
   "metadata": {},
   "source": [
    "**Data**\n",
    "\n",
    "DNA folder D1-D6 Waite et al. 2018 samples, D7 = S14, D8 = S15, D9 = S16\n",
    "\n",
    "For Waite et al. 2018 metagenomes, the three adults were Bonus, Rakiura, and Ellie. For juvenile, Tia, Waa, and Tutoko. The isolates listed on NCBI were cultured from Bonus.\n",
    "\n",
    "All three DNA samples from OG4727 were cloacitis-affected birds: Merv, Taeatanga and Bravo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889bfb07",
   "metadata": {},
   "source": [
    "## Trimmomatic\n",
    "\n",
    "**Trimmomatic:prep raw data**\n",
    "\n",
    "**Pre-processing**: concatenating samples files from multiple lanes\n",
    "   - concatenate the files for each read (R1 and R2, separately) from multiple lanes into single (paired) files per sample\n",
    "   - also rename files to a simpler format for downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to working directory\n",
    "cd <working_directory>\n",
    "\n",
    "# Make directory 0.raw_data\n",
    "mkdir 0.Raw_concat\n",
    "\n",
    "# Set up variables for input files path and output path\n",
    "inpath=0.Raw/fastq\n",
    "outpath=0.Raw_concat\n",
    "\n",
    "# For each of reads 1 and 2 (R1 and R2), concatenate multiple lanes (L001-L008) files into single output files, and rename based on sampleIDs\n",
    "for read in R1 R2;\n",
    "do\n",
    "    cat ${inpath}/*4727-01-55-1*_${read}_001.fastq.gz > ${outpath}/S1_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-02-55-1*_${read}_001.fastq.gz > ${outpath}/S2_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-03-55-1*_${read}_001.fastq.gz > ${outpath}/S3_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-04-55-1*_${read}_001.fastq.gz > ${outpath}/S4_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-05-55-1*_${read}_001.fastq.gz > ${outpath}/S5_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-06-55-1*_${read}_001.fastq.gz > ${outpath}/S6_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-07-55-1*_${read}_001.fastq.gz > ${outpath}/S7_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-08-55-1*_${read}_001.fastq.gz > ${outpath}/S8_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-09-55-1*_${read}_001.fastq.gz > ${outpath}/S9_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-10-55-1*_${read}_001.fastq.gz > ${outpath}/S10_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-11-55-1*_${read}_001.fastq.gz > ${outpath}/S11_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-12-55-1*_${read}_001.fastq.gz > ${outpath}/S12_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-13-55-1*_${read}_001.fastq.gz > ${outpath}/S13_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-14-25-01*_${read}_001.fastq.gz > ${outpath}/S14_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-15-25-01*_${read}_001.fastq.gz > ${outpath}/S15_${read}.fastq.gz\n",
    "    cat ${inpath}/*4727-16-25-01*_${read}_001.fastq.gz > ${outpath}/S16_${read}.fastq.gz\n",
    "done\n",
    "\n",
    "# Copy S14, S15 and S16 files to WGS 0.Raw_concat and rename D7, D8 and D9, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d405ce5",
   "metadata": {},
   "source": [
    "**Trimmomatic: run**\n",
    "\n",
    "**NOTE**: \n",
    "- It's good to also include a search for a truncated version of the adapters, as sometimes these aren't picked up by trimmomatic for fastqc (this is the ILLUMINACLIP bit in the script below)\n",
    "- Set `CROP` and `MINLENGTH` to something appropriate for your data (relative to sequence length for this sequencing run)\n",
    "\n",
    "Slurm array for 9 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85b2d8",
   "metadata": {},
   "source": [
    "**DNA WGS samples**\n",
    "\n",
    "`sbatch scripts/wgs_1_trimmomatic.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_1_trimmomatic\n",
    "#SBATCH --time 00:10:00\n",
    "#SBATCH --mem=12GB\n",
    "#SBATCH --array=1-9\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH -e slurm_output/wgs_1_trimmomatic_%a_%j.err\n",
    "#SBATCH -o slurm_output/wgs_1_trimmomatic_%a_%j.out\n",
    "\n",
    "# Load module(s)\n",
    "module purge\n",
    "module load Trimmomatic/0.39-Java-1.8.0_144                \n",
    "\n",
    "# Change to working directory\n",
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Make output directory \n",
    "mkdir -p 1.Qual_filtered_trimmomatic/\n",
    "\n",
    "# Set up variables for input path and output path\n",
    "inpath=0.Raw_concat\n",
    "outpath=1.Qual_filtered_trimmomatic\n",
    "\n",
    "# Make adapter file if not already created\n",
    "if [ ! -f iua.fna ]; then\n",
    "    echo \">FastQC_adapter\" > iua.fna\n",
    "    echo \"AGATCGGAAGAG\" >> iua.fna\n",
    "fi\n",
    "           \n",
    "# Quality filter and trim \n",
    "srun trimmomatic PE -threads 10 -phred33 \\\n",
    "${inpath}/D${SLURM_ARRAY_TASK_ID}_R1.fastq.gz ${inpath}/D${SLURM_ARRAY_TASK_ID}_R2.fastq.gz \\\n",
    "${outpath}/D${SLURM_ARRAY_TASK_ID}_R1.fastq D${SLURM_ARRAY_TASK_ID}_R1.single1.fastq \\\n",
    "${outpath}/D${SLURM_ARRAY_TASK_ID}_R2.fastq D${SLURM_ARRAY_TASK_ID}_R2.single2.fastq \\\n",
    "ILLUMINACLIP:iua.fna:1:25:7 CROP:115 SLIDINGWINDOW:4:30 MINLEN:50\n",
    "\n",
    "# Tidy up the singleton reads\n",
    "cat D${SLURM_ARRAY_TASK_ID}_R1.single1.fastq D${SLURM_ARRAY_TASK_ID}_R2.single2.fastq \\\n",
    "> ${outpath}/D${SLURM_ARRAY_TASK_ID}_single.fastq\n",
    "\n",
    "rm D${SLURM_ARRAY_TASK_ID}_R1.single1.fastq D${SLURM_ARRAY_TASK_ID}_R2.single2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c1af0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b072f4",
   "metadata": {},
   "source": [
    "## Fastqc analysis of post-trimmed reads\n",
    "\n",
    "`sbatch scripts/wgs_1_qc_fastqc.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_1_qc_fastqc\n",
    "#SBATCH --time 01:30:00\n",
    "#SBATCH --mem 2GB\n",
    "#SBATCH --array=1-9\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --cpus-per-task 2\n",
    "#SBATCH -e slurm_output/wgs_1_qc_fastqc_%a_%j.err\n",
    "#SBATCH -o slurm_output/wgs_1_qc_fastqc_%a_%j.out\n",
    "\n",
    "# Set up working directories\n",
    "cd <working_directory>/WGS/\n",
    "mkdir -p 1.Qual_filtered_trimmomatic/fastqc/\n",
    "\n",
    "# load modules\n",
    "module load FastQC/0.11.9\n",
    "module load MultiQC/1.9-gimkl-2020a-Python-3.8.2\n",
    "\n",
    "# Run fastqc on each sample           \n",
    "srun fastqc \\\n",
    "-o 1.Qual_filtered_trimmomatic/fastqc/ \\\n",
    "1.Qual_filtered_trimmomatic/D${SLURM_ARRAY_TASK_ID}_R1.fastq 1.Qual_filtered_trimmomatic/D${SLURM_ARRAY_TASK_ID}_R2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69fa8ac",
   "metadata": {},
   "source": [
    "**MultiQC**\n",
    "\n",
    "`sbatch scripts/wgs_1_qc_multiqc.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_1_qc_multiqc\n",
    "#SBATCH --time 00:10:00\n",
    "#SBATCH --mem 1GB\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --cpus-per-task 2\n",
    "#SBATCH -e slurm_output/wgs_1_qc_multiqc_%a_%j.err\n",
    "#SBATCH -o slurm_output/wgs_1_qc_multiqc_%a_%j.out\n",
    "\n",
    "# Set up working directories\n",
    "cd <working_directory>/WGS/\n",
    "mkdir -p 1.Qual_filtered_trimmomatic/fastqc/\n",
    "\n",
    "# load modules\n",
    "module load FastQC/0.11.9\n",
    "module load MultiQC/1.9-gimkl-2020a-Python-3.8.2\n",
    "\n",
    "# Run multiqc to generate report for all samples\n",
    "srun multiqc -f \\\n",
    "-o 1.Qual_filtered_trimmomatic/fastqc/ \\\n",
    "1.Qual_filtered_trimmomatic/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8898d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf251b2",
   "metadata": {},
   "source": [
    "## Filter out host sequences\n",
    "\n",
    "**Preamble**\n",
    "\n",
    "Metagenome data derived from microbial communities associated with a host should ideally be filtered to remove any reads originating from host DNA. This may improve the quality and efficiency of downstream data processing, and is also an important consideration when working with metagenomes that may include data of a sensitive nature (and which may also need to be removed prior to making the data publicly available e.g. kākāpō). This is especially important for any studies involving human subjects or those involving samples derived from taonga species.\n",
    "\n",
    "There are several approaches that can be used to achieve this. The general principle is to map your reads to a reference genome (e.g. human genome) and remove those reads that map to the reference from the dataset.\n",
    "\n",
    "\n",
    "Here, we will map our reads against a masked kākāpō genome which is processed to hide sections that: are presumbed microbial contaminant in the reference; have high homology to microbial genes/genomes (e.g. ribosomes); or those that are of low complexity. This ensures that reads that would normally map to these sections of the kākāpō genome are not removed from the dataset (as genuine microbial reads that we wish to retain might also map to these regions), while all reads mapping to the rest of the genome are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de34a0",
   "metadata": {},
   "source": [
    "**Create masked kākāpō genome**\n",
    "\n",
    "Download the reference genome from NCBI: https://www.ncbi.nlm.nih.gov/assembly/GCF_004027225.2/\n",
    "\n",
    "`sbatch scripts/wgs_1_masked_genome.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_1_maked_genome\n",
    "#SBATCH --time 02:00:00\n",
    "#SBATCH --mem 23GB\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --cpus-per-task 1\n",
    "#SBATCH -e slurm_output/wgs_1_hostfilt_mapping_index_%j.err\n",
    "#SBATCH -o slurm_output/wgs_1_hostfilt_mapping_index_%j.out\n",
    "\n",
    "# Set up working directories\n",
    "cd <working_directory>\n",
    "\n",
    "# Load BBMap module\n",
    "module purge\n",
    "module load BBMap/38.81-gimkl-2020a\n",
    "\n",
    "# Build BBMap index of reference genome\n",
    "srun bbmaask.sh in=ncbi-genomes-2021-12-11/GCF_004027225.2_bStrHab1.2.pri_genomic.fna.gz out=masked_kakapo_genome.fa entropy=0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56096cc7",
   "metadata": {},
   "source": [
    "**Host filtering: Build BBMap index**\n",
    "\n",
    "`sbatch scripts/wgs_1_hostfilt_mapping_index.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_1_hostfilt_mapping_index\n",
    "#SBATCH --time 00:10:00\n",
    "#SBATCH --mem 23GB\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --cpus-per-task 1\n",
    "#SBATCH -e slurm_output/wgs_1_hostfilt_mapping_index_%j.err\n",
    "#SBATCH -o slurm_output/wgs_1_hostfilt_mapping_index_%j.out\n",
    "\n",
    "# Set up working directories\n",
    "mkdir -p <working_directory>/WGS/1.host_filtered/\n",
    "cd <working_directory>/WGS/1.host_filtered/\n",
    "\n",
    "# Load BBMap module\n",
    "module purge\n",
    "module load BBMap/38.81-gimkl-2020a\n",
    "\n",
    "# Build BBMap index of reference genome\n",
    "srun bbmap.sh ref=<working_directory>/masked_kakapo_genome.fa.gz -Xmx23g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433bd7c",
   "metadata": {},
   "source": [
    "**Host filtering: per-sample BBMap read mapping, slurm array**\n",
    "\n",
    "Note:\n",
    "- This step outputs fastq files where reads that map to the reference genome have been filtered out.\n",
    "- The output from `outu` is the filtered file for downstream use.\n",
    "- Host filtering here is run as a two step process for each sample: first, on the paired reads (R1 and R2), and then again for the unpaired (single) reads file.\n",
    "- The parameters are set based on the recomendations for host filtering outlined [here](https://www.seqanswers.com/forum/bioinformatics/bioinformatics-aa/37175-introducing-removehuman-human-contaminant-removal?t=42552)\n",
    "\n",
    "`sbatch scripts/wgs_1_hostfilt_mapping.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347be0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_1_hostfilt_mapping\n",
    "#SBATCH --time 00:45:00\n",
    "#SBATCH --mem 28GB\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --array=1-9\n",
    "#SBATCH --cpus-per-task 32\n",
    "#SBATCH -e slurm_output/wgs_1_hostfilt_mapping_%a_%j.err\n",
    "#SBATCH -o slurm_output/wgs_1_hostfilt_mapping_%a_%j.out\n",
    "\n",
    "# Set up working directories\n",
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Load BBMap module\n",
    "module load BBMap/38.81-gimkl-2020a\n",
    "\n",
    "## Run bbmap\n",
    "\n",
    "# Paired reads (R1 and R2)\n",
    "srun bbmap.sh -Xmx28g -t=32 \\\n",
    "minid=0.95 maxindel=3 bwr=0.16 bw=12 quickmatch fast minhits=2 qtrim=rl trimq=10 untrim \\\n",
    "in1=1.Qual_filtered_trimmomatic/D${SLURM_ARRAY_TASK_ID}_R1.fastq \\\n",
    "in2=1.Qual_filtered_trimmomatic/D${SLURM_ARRAY_TASK_ID}_R2.fastq \\\n",
    "outu1=1.host_filtered/D${SLURM_ARRAY_TASK_ID}_R1_hostFilt.fastq \\\n",
    "outu2=1.host_filtered/D${SLURM_ARRAY_TASK_ID}_R2_hostFilt.fastq\n",
    "\n",
    "# Unpaired (single) reads\n",
    "srun bbmap.sh -Xmx28g -t=32 \\\n",
    "minid=0.95 maxindel=3 bwr=0.16 bw=12 quickmatch fast minhits=2 qtrim=rl trimq=10 untrim \\\n",
    "in=1.Qual_filtered_trimmomatic/D${SLURM_ARRAY_TASK_ID}_single.fastq \\\n",
    "outu=1.host_filtered/D${SLURM_ARRAY_TASK_ID}_single_hostFilt.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a92672",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d267cb7",
   "metadata": {},
   "source": [
    "## Checking library sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcea500",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <working_directory>\n",
    "\n",
    "# All raw files\n",
    "for file in 0.Raw_concat/*; do\n",
    "    echo ${file} \n",
    "done\n",
    "for file in 0.Raw_concat/*; do\n",
    "    zgrep -c '@' ${file} \n",
    "done\n",
    "\n",
    "# Trimmed files\n",
    "for file in 1.Qual_filtered_trimmomatic/*.fastq; do\n",
    "    echo ${file}\n",
    "done\n",
    "for file in 1.Qual_filtered_trimmomatic/*.fastq; do\n",
    "    grep -c '@' ${file} \n",
    "done\n",
    "\n",
    "# Host filtered files\n",
    "for file in 1.host_filtered/*.fastq; do\n",
    "    echo ${file}\n",
    "done\n",
    "for file in 1.host_filtered/*.fastq; do\n",
    "    grep -c '@' ${file} \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101092c6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d059bd",
   "metadata": {},
   "source": [
    "## Assembly\n",
    "\n",
    "Assembly via metaSPAdes    \n",
    "\n",
    "Single- versus co-assemblies\n",
    "\n",
    "- Depending on your study question and available time and computational resources, you may wish to do single assemblies (i.e. a separate assembly per sample), or some variety of co-assemblies (e.g. full co-assembly (all samples together), or mini co-assemblies (e.g. one assembly of samples from group A and a separate assembly of samples from group B)).\n",
    "- NOTE:\n",
    "    - Individual assemblies per sample may result in better assemblies overall\n",
    "    - Alternatively, co-assemblies may be better at assembling rarer taxa that occur in > 1 sample\n",
    "    - If following up with read mapping (e.g. mapping WTS reads back to assembled WGS contigs), any more than one single co-assembly at this stage will require a subsequent step to dereplicate assembled contigs across the multiple assemblies. (n.b. You can also combine both individual assemblies and co-assemblies and dereplicate across all assemblies).\n",
    "- For co-assemblies, input files can be concatenated together via `cat`, e.g:\n",
    "    - `cat sample1_R1.fastq.gz sample2_R1.fastq.gz sample3_R1.fastq.gz sample4_R1.fastq.gz > for_assembly_A_R1.fastq.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c40d75",
   "metadata": {},
   "source": [
    "**Run co-assembly**\n",
    "\n",
    "Concatenate reads for assembly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a895939",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <working_directory>/WGS/\n",
    "\n",
    "mkdir -p 2.assembly_spades/infiles_concat\n",
    "\n",
    "cat 1.host_filtered/*_R1_hostFilt.fastq > 2.assembly_spades/infiles_concat/host_filtered_reads_R1.fastq\n",
    "cat 1.host_filtered/*_R2_hostFilt.fastq > 2.assembly_spades/infiles_concat/host_filtered_reads_R2.fastq\n",
    "cat 1.host_filtered/*_single_hostFilt.fastq > 2.assembly_spades/infiles_concat/host_filtered_reads_single.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32399a97",
   "metadata": {},
   "source": [
    "`sbatch scripts/wgs_2.coassembly_spades.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed32fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_2.coassembly_spades_HF\n",
    "#SBATCH --time 48:00:00\n",
    "#SBATCH --mem=200GB\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --hint nomultithread\n",
    "#SBATCH --cpus-per-task=18\n",
    "#SBATCH -e slurm_output/wgs_2.coassembly_spades_HF_%j.err\n",
    "#SBATCH -o slurm_output/wgs_2.coassembly_spades_HF_%j.out\n",
    "\n",
    "# Load module(s)\n",
    "module purge\n",
    "module load SPAdes/3.13.1-gimkl-2018b\n",
    "\n",
    "# Set up working directory\n",
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Make output directory \n",
    "mkdir -p 2.assembly_spades/coassembly\n",
    "\n",
    "# Run metaSPAdes\n",
    "srun spades.py --meta -t 18 -m 200 \\\n",
    "-1 2.assembly_spades/infiles_concat/host_filtered_reads_R1.fastq \\\n",
    "-2 2.assembly_spades/infiles_concat/host_filtered_reads_R2.fastq \\\n",
    "-s 2.assembly_spades/infiles_concat/host_filtered_reads_single.fastq \\\n",
    "-o 2.assembly_spades/coassembly/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39618c11",
   "metadata": {},
   "source": [
    "**Run individual assemblies as slurm array**\n",
    "\n",
    "*NB: D1 did not assemble despite multiple attempts*\n",
    "\n",
    "`sbatch scripts/wgs_2.assembly_spades.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4af2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_2.assembly_spades_HF\n",
    "#SBATCH --time 40:00:00\n",
    "#SBATCH --mem=150GB\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --array=1-9\n",
    "#SBATCH --cpus-per-task=12\n",
    "#SBATCH -e slurm_output/wgs_2.assembly_spades_HF_%a_%j.err\n",
    "#SBATCH -o slurm_output/wgs_2.assembly_spades_HF_%a_%j.out\n",
    "\n",
    "# Load module(s)\n",
    "module purge\n",
    "module load SPAdes/3.13.1-gimkl-2018b\n",
    "\n",
    "# Set up working directory\n",
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Make output directory \n",
    "mkdir -p 2.assembly_spades/indv_assembly\n",
    "\n",
    "# Run metaSPAdes\n",
    "srun spades.py --meta -t 12 -m 150 \\\n",
    "-1 1.host_filtered/D${SLURM_ARRAY_TASK_ID}_R1_hostFilt.fastq \\\n",
    "-2 1.host_filtered/D${SLURM_ARRAY_TASK_ID}_R2_hostFilt.fastq \\\n",
    "-s 1.host_filtered/D${SLURM_ARRAY_TASK_ID}_single_hostFilt.fastq \\\n",
    "-o 2.assembly_spades/indv_assembly/D${SLURM_ARRAY_TASK_ID}_assembly/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a340333",
   "metadata": {},
   "source": [
    "***Assembly statistics via BBMap's stats.sh script***\n",
    "\n",
    "\n",
    "A key thing to take note of from the output of this script is the `contig N/L50` (which are output the wrong way around)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module purge\n",
    "module load BBMap/38.73-gimkl-2018b\n",
    "\n",
    "cd <working_directory>/WGS/\n",
    "\n",
    "## Run stats.sh\n",
    "\n",
    "# Individual assemblies\n",
    "for i in {1..9}; do\n",
    "     stats.sh in=2.assembly_spades/indv_assembly/D${i}_assembly/scaffolds.fasta\n",
    "done\n",
    "\n",
    "# Co-assembly\n",
    "stats.sh in=2.assembly_spades/coassembly/scaffolds.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae59ad",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36518317",
   "metadata": {},
   "source": [
    "## Dereplicating contigs across multiple assemblies\n",
    "\n",
    "If you have more than one single co-assembly, it is necessary to dereplicate similar and/or identical contigs across the multiple assemblies to obtain a single set of dereplicated contigs for downstream steps such as read mapping etc.\n",
    "Below will cover using BBMap's dedupe.sh.\n",
    "\n",
    "**Caveat:**\n",
    "\n",
    "Properly dereplicating contigs across assemblies is a generally tricky process in the case where contigs derived from the same genome overlap but have overhangs off both ends.\n",
    "\n",
    "e.g. the following two contigs may be from the same genome (or closely related genomes), in which case you would ideally like them to be dereplicated into a single representative contig (esp. if there is a gene or genes in the overlap region that you want to map reads back to; retaining both contigs results in splitting the reads over both contigs, falsely deflating the overall coverage count for that particular gene).\n",
    "\n",
    "`----------------TACC[...]AGATCAAGGACCAACTGGACC[...]\\ [...]ATTGAAGTAGCTACC[...]AGATCAAG------------------`\n",
    "\n",
    "However, this remains problematic to resolve, and so for now we will accept that the dereplication process might not be ideal.\n",
    "\n",
    "One upshot is that if you included individual sample assemblies and also a co-assembly across all samples (or multiple mini-coassemblies) you likely increase the odds of having one assembled contig from one of them that covers the full expanse of other contigs from this genome(s) (i.e. ideally the other related contigs are contained within a longer one).\n",
    "\n",
    "**Concatenate contigs from multiple assemblies**\n",
    "\n",
    "Note:\n",
    "\n",
    "- On rare occasions, different assemblies can contain contigs with identical names, which can be problematic for downstream processing.\n",
    "- Here we will modify contig headers to include the assembly ID (e.g. sampleID or coassemblyID) before concatenating together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <working_directory>/WGS/\n",
    "\n",
    "mkdir -p 2.assembly_spades/dedupe/infiles/assemblies/\n",
    "\n",
    "## Copy assembly files and add binID to contig headers \n",
    "\n",
    "# Co-assembly\n",
    "cp 2.assembly_spades/coassembly/scaffolds.fasta 2.assembly_spades/dedupe/infiles/assemblies/coassembly.fna\n",
    "\n",
    "# Individual sample assemblies\n",
    "for i in {2..9}; do\n",
    "   cp 2.assembly_spades/indv_assembly/D${i}_assembly/scaffolds.fasta 2.assembly_spades/dedupe/infiles/assemblies/D${i}_assembly.fna\n",
    "done\n",
    "\n",
    "# Modify contigIDs to include assemblyIDs\n",
    "for file in 2.assembly_spades/dedupe/infiles/assemblies/*.fna; do\n",
    "    assemblyID=$(basename ${file} .fna)\n",
    "    sed -i -e \"s/>/>${assemblyID}_/g\" ${file}\n",
    "done\n",
    "\n",
    "# Concatenate assembled contigs into all_contigs.fna\n",
    "cat 2.assembly_spades/dedupe/infiles/assemblies/*.fna > 2.assembly_spades/dedupe/infiles/all_contigs.fna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4914f99",
   "metadata": {},
   "source": [
    "**Dereplicate contigs across all assemblies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bd1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Load module\n",
    "module purge\n",
    "module load BBMap/38.73-gimkl-2018b\n",
    "\n",
    "# Run dedupe on all assemblies\n",
    "dedupe.sh threads=8 minidentity=100 overwrite=t \\\n",
    "in=2.assembly_spades/dedupe/infiles/all_contigs.fna \\\n",
    "out=2.assembly_spades/dedupe/dereplicated_contigs.fna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12578939",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8bd2a",
   "metadata": {},
   "source": [
    "## Gene prediction of dereplicated contig set\n",
    "\n",
    "Predict genes in dereplicated contigs via prodigal\n",
    "\n",
    "Note: If of interest, you can also predict rRNA genes via metaxa2 (although these tend to be poorly assembled), and/or tRNA genes via aragorn.\n",
    "\n",
    "`sbatch scripts/wgs_3_gene_calling.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_3_gene_calling\n",
    "#SBATCH --time 00:30:00\n",
    "#SBATCH --mem=1GB\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=20\n",
    "#SBATCH -e slurm_output/wgs_3_gene_calling_%j.err\n",
    "#SBATCH -o slurm_output/wgs_3_gene_calling_%j.out\n",
    "\n",
    "# Set up working directories\n",
    "cd <working_directory>/WGS/\n",
    "mkdir -p 3.gene_calling/\n",
    "\n",
    "# Load modules etc.\n",
    "module purge\n",
    "module load prodigal/2.6.3-GCCcore-7.4.0\n",
    "\n",
    "# Set up variables\n",
    "infile='2.assembly_spades/dedupe/dereplicated_contigs.fna'\n",
    "outfile='3.gene_calling/dereplicated_contigs'\n",
    "\n",
    "# Prodigal\n",
    "prodigal -p meta -q -i ${infile} -a ${outfile}.prod.faa -d ${outfile}.prod.fna > /dev/null\n",
    "\n",
    "# Strip metadata from prodigal output (problematic for some downstream applications)\n",
    "cut -f1 -d \" \" ${outfile}.prod.faa > ${outfile}.prod.faa.nometa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677906f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d71434d",
   "metadata": {},
   "source": [
    "## Gene annotation of dereplicated contig set\n",
    "\n",
    "Annotate predicted genes in dereplicated contigs via searches against UniRef, UniProt, Pfam, TIGRfam, and KEGG\n",
    "\n",
    "Note:\n",
    "\n",
    "- The databases used below are stored within a Handley group project directory. If you do not have access to this directory, you will need to download and/or build the databases separately.\n",
    "- The Handley group has a paid licence for the full version of *KEGG* used here. If you are not affiliated with the Handley group, you will need to purchase a licence or use a free alternative.\n",
    "- The full version of *usearch* similarly requires a paid licence. An alternative is to run the search via *DIAMOND*.\n",
    "\n",
    "\n",
    "`sbatch scripts/wgs_4_gene_annotation.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_4_gene_annotation\n",
    "#SBATCH --time 05:00:00\n",
    "#SBATCH --mem 250GB\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=36\n",
    "#SBATCH -e slurm_output/wgs_4_gene_annotation_%j.err\n",
    "#SBATCH -o slurm_output/wgs_4_gene_annotation_%j.out\n",
    "\n",
    "# Set up directories\n",
    "cd <working_directory>/WGS/\n",
    "mkdir -p 4.gene_annotation/\n",
    "\n",
    "# Load modules and/or software paths\n",
    "module purge\n",
    "module load HMMER/3.1b2-gimkl-2017a\n",
    "export PATH=\"/nesi/project/uoa02469/Software/usearch:$PATH\"\n",
    "# Database path masks\n",
    "export KEGG_udb=\"/nesi/project/uoa02469/Databases/KEGG/kegg.20180523.usearch.udb\"\n",
    "export UNIREF100_udb=\"/nesi/project/uoa02469/Databases/UniRef100/uniref100.20181026.usearch.udb\"\n",
    "export UNIPROT_udb=\"/nesi/project/uoa02469/Databases/UniProt/uniprot.20181026.usearch.udb\"\n",
    "export PFAM_hmm=\"/nesi/project/uoa02469/Databases/PFAM_v32.0/Pfam-A.hmm\"\n",
    "export TIGRFAM_hmm=\"/nesi/project/uoa02469/Databases/TIGRfam_v14.0/TIGRfam.hmm\"\n",
    "\n",
    "# set up in/out variables\n",
    "INFILE='3.gene_calling/dereplicated_contigs.prod.faa.nometa'\n",
    "OUTFILE='4.gene_annotation/dereplicated_contigs'\n",
    "\n",
    "# Run annotations\n",
    "srun usearch9.0.2132_i86linux64 -id 0.5 -evalue 0.001 -maxhits 10 -top_hits_only -threads 36 \\\n",
    "                            -db ${KEGG_udb} -userfields query+target+tlo+thi+id+bits+evalue \\\n",
    "                            -usearch_global ${INFILE} \\\n",
    "                            -userout ${OUTFILE}.kegg\n",
    "\n",
    "srun usearch9.0.2132_i86linux64 -id 0.5 -evalue 0.001 -maxhits 10 -top_hits_only -threads 36 \\\n",
    "                            -db ${UNIREF100_udb} -userfields query+target+tlo+thi+id+bits+evalue \\\n",
    "                            -usearch_global ${INFILE} \\\n",
    "                            -userout ${OUTFILE}.uniref100\n",
    "\n",
    "srun usearch9.0.2132_i86linux64 -id 0.5 -evalue 0.001 -maxhits 10 -top_hits_only -threads 36 \\\n",
    "                            -db ${UNIPROT_udb} -userfields query+target+tlo+thi+id+bits+evalue \\\n",
    "                            -usearch_global ${INFILE} \\\n",
    "                            -userout ${OUTFILE}.uniprot\n",
    "\n",
    "srun hmmsearch --tblout ${OUTFILE}.pfam -E 1e-3 --cpu 25 ${PFAM_hmm} ${INFILE} > /dev/null\n",
    "srun hmmsearch --tblout ${OUTFILE}.tigrfam -E 1e-3 --cpu 25 ${TIGRFAM_hmm} ${INFILE} > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856b2fb",
   "metadata": {},
   "source": [
    "**Aggregate annotations**\n",
    "\n",
    "Run through `annotationAggregator.py` to generate summary annotation table\n",
    "\n",
    "Note:\n",
    "\n",
    "- `annotationAggregator.py` developed by David Waite\n",
    "- Due to licencing requirements (e.g. for *KEGG*), `annotationAggregator.py` is only available for researchers affiliated with the Handley group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0473c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Load modules and/or software paths\n",
    "module purge\n",
    "source /nesi/project/uoa02469/Software/PipelineSetup/annotation.sh\n",
    "\n",
    "# Run aggregator - unbinned contigs\n",
    "annotationAggregator.py --toponly --ident 30 --coverage 70 \\\n",
    "-p 4.gene_annotation/dereplicated_contigs.pfam \\\n",
    "-i 4.gene_annotation/dereplicated_contigs.tigrfam \\\n",
    "-r 4.gene_annotation/dereplicated_contigs.uniref100 \\\n",
    "-u 4.gene_annotation/dereplicated_contigs.uniprot \\\n",
    "-k 4.gene_annotation/dereplicated_contigs.kegg \\\n",
    "2.assembly_spades/dedupe/dereplicated_contigs.fna \\\n",
    "3.gene_calling/dereplicated_contigs.prod.fna \\\n",
    "3.gene_calling/dereplicated_contigs.prod.faa \\\n",
    "4.gene_annotation/dereplicated_contigs.annotation_summary\n",
    "\n",
    "#---------------------------------------------------------#\n",
    "\n",
    "# Load software\n",
    "module purge\n",
    "export PATH=\"/nesi/project/uoa02469/Software/annotationaggregator_v0.1:$PATH\"\n",
    "# Working directory\n",
    "cd <working_directory>/WGS/\n",
    "### Run aggregator\n",
    "# aggregate all datasets except lsu\n",
    "annotationAggregator.py --toponly --ident 30 --coverage 70 \\\n",
    "-p 4.gene_annotation/dereplicated_contigs.pfam \\\n",
    "-i 4.gene_annotation/dereplicated_contigs.tigrfam \\\n",
    "-r 4.gene_annotation/dereplicated_contigs.uniref100 \\\n",
    "-u 4.gene_annotation/dereplicated_contigs.uniprot \\\n",
    "-k 4.gene_annotation/dereplicated_contigs.kegg \\\n",
    "2.assembly_spades/dedupe/dereplicated_contigs.fna \\\n",
    "3.gene_calling/dereplicated_contigs.prod.fna \\\n",
    "3.gene_calling/dereplicated_contigs.prod.faa \\\n",
    "4.gene_annotation/dereplicated_contigs.annotation_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6a851",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2b8a6",
   "metadata": {},
   "source": [
    "## Read mapping\n",
    "\n",
    "**BBMap: Generate read mapping index**\n",
    "\n",
    "Generate read mapping index based on assembled contigs from wgs data.\n",
    "\n",
    "*NOTE: if altering the memory allocation, this needs to be modified in both the slurm header and the bbmap.sh script*\n",
    "\n",
    "`sbatch scripts/wgs_5_read_mapping_index.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31202017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_5_read_mapping_index\n",
    "#SBATCH --time 00:02:00\n",
    "#SBATCH --mem 2GB\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --cpus-per-task 30\n",
    "#SBATCH -e slurm_output/wgs_5_read_mapping_index_%j.err\n",
    "#SBATCH -o slurm_output/wgs_5_read_mapping_index_%j.out\n",
    "\n",
    "# Set up directories\n",
    "cd <working_directory>/WGS/\n",
    "mkdir -p 5.read_mapping\n",
    "cd 5.read_mapping\n",
    "\n",
    "# Load dependencies\n",
    "module purge\n",
    "module load BBMap/38.90-gimkl-2020a\n",
    "\n",
    "# Build index - SPAdes assemblies\n",
    "bbmap.sh -Xmx2g ref=<working_directory>/2.assembly_spades/dedupe/dereplicated_contigs.fna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61264d",
   "metadata": {},
   "source": [
    "**BBMap: per-sample WGS read mapping, slurm array**\n",
    "\n",
    "`sbatch scripts/wgs_6_read_mapping.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wgs_6_read_mapping\n",
    "#SBATCH --time 00:20:00\n",
    "#SBATCH --mem 70GB\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --array=1-9\n",
    "#SBATCH --cpus-per-task 30\n",
    "#SBATCH -e slurm_output/wgs_6_read_mapping_%a_%j.err\n",
    "#SBATCH -o slurm_output/wgs_6_read_mapping_%a_%j.out\n",
    "\n",
    "# Set up working directories\n",
    "cd <working_directory>/WGS/5.read_mapping/\n",
    "mkdir -p wgs/\n",
    "\n",
    "# Load dependencies\n",
    "module purge\n",
    "module load BBMap/38.90-gimkl-2020a\n",
    "module load SAMtools/1.10-GCC-9.2.0\n",
    "\n",
    "# in/out varibles\n",
    "WGS_READ_DIR='<working_directory>/WGS/1.host_filtered'\n",
    "OUTPATH='wgs'\n",
    "\n",
    "# Run read mapping\n",
    "srun bbmap.sh \\\n",
    "t=30 -Xmx70g ambiguous=best minid=0.95 \\\n",
    "in1=${WGS_READ_DIR}/D${SLURM_ARRAY_TASK_ID}_R1_hostFilt.fastq \\\n",
    "in2=${WGS_READ_DIR}/D${SLURM_ARRAY_TASK_ID}_R2_hostFilt.fastq \\\n",
    "covstats=${OUTPATH}/D${SLURM_ARRAY_TASK_ID}.wgs.covstats.txt \\\n",
    "statsfile=${OUTPATH}/D${SLURM_ARRAY_TASK_ID}.wgs.statsfile.txt \\\n",
    "out=${OUTPATH}/D${SLURM_ARRAY_TASK_ID}.wgs.sam\n",
    "\n",
    "# Convert sam to bam\n",
    "samtools sort -@ 10 -o ${OUTPATH}/D${SLURM_ARRAY_TASK_ID}.wgs.bam ${OUTPATH}/D${SLURM_ARRAY_TASK_ID}.wgs.sam\n",
    "\n",
    "# Run pileup to extract read counts\n",
    "pileup.sh \\\n",
    "in=${OUTPATH}/D${SLURM_ARRAY_TASK_ID}.wgs.sam \\\n",
    "rpkm=${OUTPATH}/D${SLURM_ARRAY_TASK_ID}.wgs.covstats_pileup.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055db188",
   "metadata": {},
   "source": [
    "**BBMap: per-sample WTS read mapping, slurm array**\n",
    "\n",
    "Map RNA transcripts (filtered reads; see WTS pipeline) to metagenome contigs\n",
    "\n",
    "`sbatch scripts/wts_6_read_mapping.sl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -A uoa03068\n",
    "#SBATCH -J wts_6_read_mapping\n",
    "#SBATCH --time 00:20:00\n",
    "#SBATCH --mem 20GB\n",
    "#SBATCH --ntasks 1\n",
    "#SBATCH --array=1-13\n",
    "#SBATCH --cpus-per-task 30\n",
    "#SBATCH -e slurm_output/wts_6_read_mapping%a_%j.err\n",
    "#SBATCH -o slurm_output/wts_6_read_mapping%a_%j.out\n",
    "\n",
    "# Set up working directories\n",
    "cd <working_directory>/WGS/5.read_mapping/\n",
    "mkdir -p wts/\n",
    "\n",
    "# Load dependencies\n",
    "module purge\n",
    "module load BBMap/38.90-gimkl-2020a\n",
    "module load SAMtools/1.10-GCC-9.2.0\n",
    "\n",
    "# in/out varibles\n",
    "READ_DIR='<working_directory>/WTS/1.host_filtered' #see WTS pipeline for processed metatranscriptomic reads\n",
    "OUTPATH='wts'\n",
    "\n",
    "# Run read mapping\n",
    "srun bbmap.sh \\\n",
    "t=30 -Xmx20g ambiguous=best minid=0.95 \\\n",
    "in1=${READ_DIR}/S${SLURM_ARRAY_TASK_ID}_R1_hostFilt.fastq \\\n",
    "in2=${READ_DIR}/S${SLURM_ARRAY_TASK_ID}_R2_hostFilt.fastq \\\n",
    "covstats=${OUTPATH}/S${SLURM_ARRAY_TASK_ID}.wts.covstats.txt \\\n",
    "statsfile=${OUTPATH}/S${SLURM_ARRAY_TASK_ID}.wts.statsfile.txt \\\n",
    "out=${OUTPATH}/S${SLURM_ARRAY_TASK_ID}.wts.sam\n",
    "\n",
    "# Convert sam to bam\n",
    "samtools sort -@ 10 -o ${OUTPATH}/S${SLURM_ARRAY_TASK_ID}.wts.bam ${OUTPATH}/S${SLURM_ARRAY_TASK_ID}.wts.sam\n",
    "\n",
    "# Run pileup to extract read counts\n",
    "pileup.sh \\\n",
    "in=${OUTPATH}/S${SLURM_ARRAY_TASK_ID}.wts.sam \\\n",
    "rpkm=${OUTPATH}/S${SLURM_ARRAY_TASK_ID}.wts.covstats_pileup.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9031c83",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e600e",
   "metadata": {},
   "source": [
    "## Assign mapped reads to predicted genes\n",
    "\n",
    "Assign mapped DNA and RNA reads to predicted genes via featurecounts\n",
    "\n",
    "**Generate gene_coords SAF file**\n",
    "\n",
    "Extract coordinates of prodigal predicted genes and generate gene_coords SAF file (required for downstream use with *featurecounts*)\n",
    "\n",
    "Note: code here c/o David Waite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Load python\n",
    "module purge\n",
    "module load Python/3.8.2-gimkl-2020a\n",
    "python3\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "## Prodigal calls for prok bins\n",
    "# Import the prodigal file, and extract sequence names/metadata\n",
    "prodigal_file = '3.gene_calling/dereplicated_contigs.prod.faa'\n",
    "prodigal_headers = [ line.replace('>', '') for line in open(prodigal_file, 'r') if '>' in line ]\n",
    "\n",
    "# Define a function for splitting the metadata into row-wise dictionaries\n",
    "def prodigal_to_dict(file_line):\n",
    "    gene_name, start, stop, orientation, *rest = file_line.split('#')\n",
    "    return { 'GeneID': gene_name.strip(), \n",
    "             'Chr': re.sub('_\\d+$', '', gene_name.strip()),\n",
    "             'Start': start.strip(),\n",
    "             'End': stop.strip(),\n",
    "             'Strand': '+' if int(orientation) == 1 else '-' }\n",
    "\n",
    "# Convert the prodigal lines into a DataFrame\n",
    "prodigal_buffer = [ prodigal_to_dict(header) for header in prodigal_headers ]\n",
    "prok_df = pd.DataFrame(prodigal_buffer)\n",
    "\n",
    "# Order the columns, and save the file\n",
    "prok_df = prok_df[ ['GeneID', 'Chr', 'Start', 'End', 'Strand' ] ]\n",
    "prok_df.to_csv('3.gene_calling/dereplicated_contigs.gene_coords.SAF', sep='\\t', index=False)\n",
    "\n",
    "quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc14d48",
   "metadata": {},
   "source": [
    "**featureCounts: WGS read mapping data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e09b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up working directories\n",
    "cd <working_directory>/WGS/\n",
    "mkdir -p 5.read_mapping/wgs/featurecounts/\n",
    "\n",
    "# Run featureCounts\n",
    "/nesi/project/uoa02469/Software/subread-1.6.3-source/bin/featureCounts \\\n",
    "-p -T 6 -t exon -F SAF \\\n",
    "-a 3.gene_calling/dereplicated_contigs.gene_coords.SAF \\\n",
    "-o 5.read_mapping/wgs/featurecounts/wgs_dereplicated_contigs.gene_counts.txt \\\n",
    "5.read_mapping/wgs/*.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f47ecf",
   "metadata": {},
   "source": [
    "**featureCounts: WTS read mapping data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3991c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up working directories\n",
    "cd <working_directory>/WGS/\n",
    "mkdir -p 5.read_mapping/wts/featurecounts/\n",
    "\n",
    "# Run featureCounts\n",
    "/nesi/project/uoa02469/Software/subread-1.6.3-source/bin/featureCounts \\\n",
    "-p -T 6 -t exon -F SAF \\\n",
    "-a 3.gene_calling/dereplicated_contigs.gene_coords.SAF \\\n",
    "-o 5.read_mapping/wts/featurecounts/wts_dereplicated_contigs.gene_counts.txt \\\n",
    "5.read_mapping/wts/*.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a479d6a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6c213",
   "metadata": {},
   "source": [
    "## Per-sample coverage calculations\n",
    "\n",
    "Per-sample coverages can be calculated at:\n",
    "\n",
    "- genome level (e.g. across multiple binned contigs)\n",
    "- contig level (e.g. for viral genomes represented by a single contig)\n",
    "- gene level\n",
    "\n",
    "`summarise_counts.py` (and associated `script summarise_counts.R`) was developed by Michael Hoggard and can output each of these sets of summaries depending on the input data (*pileup* or *featurecounts*) and output options.\n",
    "\n",
    "However, genome and contig-level summaries are probably not useful here (since we have not binned contigs into putative genomes and/or are not working with viral genomes on a single contig), so we will only generate gene-level coverage stats based on the output from featurecounts.\n",
    "\n",
    "Below, gene-level coverage summary outputs will be generated for:\n",
    "\n",
    "- WGS read mapping (indicative of DNA data gene-read-coverages)\n",
    "- WTS read mapping (indicative of RNA data gene-read-coverages)\n",
    "\n",
    "\n",
    "*NOTE: if you are including lib.size, you will need to generate two separate mapping files, one for wgs data and one for wts data (as the library size counts will differ)*\n",
    "\n",
    "- Required columns (spelling of column headers matters)\n",
    "    - `sampleID`: unique strings that identify each sample (these unique strings must also be present in the counts files file names)\n",
    "    - `group`: treatment groups for each sample (e.g. 'treatment', 'control'; or 'groupA', 'groupB', 'groupC' etc.)\n",
    "- Optional column\n",
    "    - `lib.size`: total library size per sample (total read counts in the quality trimmed and filtered fastq files used for read mapping). (NOTE: this column is required if `--format featurecounts` and `--lib_norm total` (for `--format pileup`, this data is present in the input files output by pileup's `rpkm=...` option))\n",
    "\n",
    "\n",
    "**WGS: per-sample DNA read coverage, gene level**\n",
    "\n",
    "`summarise_counts.py` process:\n",
    "\n",
    "- Extracts DNA read counts per gene from *featurecounts* output\n",
    "- Calculates coverage normalisation based on several methods (incl. rpkm, tpm, tmm). (Choose your preferred/the most appropriate normalised dataset for your data).\n",
    "- Outputs:\n",
    "    - Summary table of per-sample coverage stats per gene\n",
    "    - Summary table of WGS read counts per sample (Read counts from trimmed filtered reads input into read mapping)\n",
    "    - Summary table of *EdgeR* analysis of \"differentially expressed genes\" (DEG) based on pairwise comparisons between sample groups\n",
    "        - In this case, this will actually be differentially *abundant* genes (rather than expressed genes), since we have input DNA read coverage rather than RNA read coverage here, but as *EdgeR* was designed with transcription data in mind its use here is tentative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab592996",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Load python and R\n",
    "module purge\n",
    "module load Python/3.8.2-gimkl-2020a\n",
    "module load R/3.6.2-gimkl-2020a\n",
    "\n",
    "# Add location of summarise_counts.py and summarise_counts.R to PATH\n",
    "export PATH=\"/nesi/project/uoa02469/custom-scripts/MikeH:$PATH\"\n",
    "\n",
    "# Run summarise_counts\n",
    "summarise_counts.py \\\n",
    "    --input '5.read_mapping/wgs/featurecounts/wgs_dereplicated_contigs.gene_counts.txt' --format featurecounts \\\n",
    "    --sample_mapping_file 5.read_mapping/wgs/wgs_sample_mapping_file.tsv \\ # old vs new data groups\n",
    "    --lib_norm total \\\n",
    "    --count_threshold 10 \\\n",
    "    --read_counts 5.read_mapping/wgs/wgs_summary_read_counts.tsv \\\n",
    "    --edger_out 5.read_mapping/wgs/wgs_summary_edgeR_glmQLFTest.tsv \\\n",
    "    --output 5.read_mapping/wgs/wgs_summary_count_table.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf392889",
   "metadata": {},
   "source": [
    "**WTS: per-sample RNA read coverage, gene level**\n",
    "\n",
    "The process here:\n",
    "\n",
    "- Extracts RNA read counts per gene from *featurecounts* output\n",
    "- Calculates coverage normalisation based on several methods (incl. rpkm, tpm, tmm). (Choose your preferred/the most appropriate normalised dataset for your data).\n",
    "- Outputs:\n",
    "    - Summary table of per-sample coverage stats per gene\n",
    "    - Summary table of WTS read counts per sample (Read counts from trimmed filtered reads input into read mapping)\n",
    "    - Summary table of *EdgeR* analysis of differentially expressed genes (DEG) based on pairwise comparisons between sample groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <working_directory>/WGS/\n",
    "\n",
    "# Load python and R\n",
    "module purge\n",
    "module load Python/3.8.2-gimkl-2020a\n",
    "module load R/3.6.2-gimkl-2020a\n",
    "\n",
    "# Run summarise_counts\n",
    "summarise_counts.py \\\n",
    "    --input '5.read_mapping/wts/featurecounts/wts_dereplicated_contigs.gene_counts.txt' --format featurecounts \\\n",
    "    --sample_mapping_file 5.read_mapping/wts/wts_sample_mapping_file.tsv \\ # cloacitis groups\n",
    "    --lib_norm total \\\n",
    "    --count_threshold 10 \\\n",
    "    --read_counts 5.read_mapping/wts/wts_summary_read_counts.tsv \\\n",
    "    --edger_out 5.read_mapping/wts/wts_summary_edgeR_glmQLFTest.tsv \\\n",
    "    --output 5.read_mapping/wts/wts_summary_count_table.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a52b6d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115af73",
   "metadata": {},
   "source": [
    "## Some key output files for downstream analyses\n",
    "\n",
    "Each of the below outputs should contain a column of shared unique gene identifiers to enable joining for downstream exploration and analyses (e.g. pairing per-sample coverage and/or differential expression results with predicted annotations results etc.)\n",
    "\n",
    "- `working/dir/4.gene_annotation/dereplicated_contigs.annotation_summary.aa` Gene annotations summary table (with amino acid sequence)\n",
    "- `working/dir/5.read_mapping/wgs/wgs_summary_count_table.tsv` WGS per-sample (normalised) gene coverage stats summary table\n",
    "- `working/dir/5.read_mapping/wgs/wgs_summary_edgeR_glmQLFTest.tsv` WGS EdgeR \"differentially ~expressed~ abundant genes\" summary table\n",
    "- `working/dir/5.read_mapping/wts/wts_summary_count_table.tsv WTS` per-sample (normalised) gene coverage stats summary table\n",
    "- `working/dir/5.read_mapping/wts/wts_summary_edgeR_glmQLFTest.tsv` WTS EdgeR \"differentially expressed genes\" summary table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7aff13",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
